{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38332bit92b6213f5c2744d0ab77ccfa3a973704",
   "display_name": "Python 3.8.3 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nIn order to properly setup my local environment:\\n- pip install boto3\\n- I had to install Spark, Java, Hadoop, and SBT on my Windows 10 machine, ADD YT LINK FOR SBT TOO!!\\nfollowing the instructions at this link: https://www.youtube.com/watch?v=g7Qpnmi0Q-s for the Java/Hadoop portion.\\n'"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "'''\n",
    "In order to properly setup my local environment:\n",
    "- pip install boto3\n",
    "- I had to install Spark, Java, Hadoop, and SBT on my Windows 10 machine, ADD YT LINK FOR SBT TOO!!\n",
    "following the instructions at this link: https://www.youtube.com/watch?v=g7Qpnmi0Q-s for the Java/Hadoop portion.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import types as T\n",
    "import  pyspark.sql.functions as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['dl.cfg']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0xe0276e8>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://DESKTOP-GJKKHSO:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to speed up parquet write\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ff8006fd3aa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhadoopConfiguration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spark.speculation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"false\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc._jsc.hadoopConfiguration().set(\"spark.speculation\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "runtime (s): 529.4650149345398\n"
    }
   ],
   "source": [
    "# Not sure that second line is faster than first\n",
    "#df = spark.read.format(\"json\").load(\"s3a://udacity-dend/song_data/*/*/*\") # runs in 13 minutes\n",
    "#df = spark.read.format(\"json\").load(file_locations)\n",
    "# df = spark.read.format(\"json\").load(songs_paths)\n",
    "# song_data = input_data+'song_data/*/*/*/*.json\n",
    "# song_data = input_data + 'song_data/*/*/*/*.json # runs in 14 minutes\n",
    "# df = spark.read.json(song_data) # runs in 14 minutes\n",
    "# df = spark.read.json(songs_paths) # runs in 26 minutes\n",
    "\n",
    "start = time.time()\n",
    "#input_data = \"s3a://udacity-dend/\"\n",
    "#song_data =  input_data + 'song_data/*/*/*/*.json'\n",
    "df = spark.read.json(\"s3a://udacity-dend/song_data/*/*/*\") # runs in 13 minutes\n",
    "end = time.time()\n",
    "print('runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pyspark.sql.dataframe.DataFrame'>\nDataFrame[artist_id: string, artist_latitude: double, artist_location: string, artist_longitude: double, artist_name: string, duration: double, num_songs: bigint, song_id: string, title: string, year: bigint]\ncount 14896\nruntime (s): 339.83151745796204\n"
    }
   ],
   "source": [
    "'''\n",
    "start = time.time()\n",
    "print(type(df))\n",
    "print(df)\n",
    "print('count', df.count())\n",
    "#df.printSchema()\n",
    "end = time.time()\n",
    "print('runtime (s):', end-start)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://dend-sparkify-amiri/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`song_id`' given input columns: [artist, auth, firstName, gender, itemInSession, lastName, length, level, location, method, page, registration, sessionId, song, start_time, status, ts, userAgent, userId];;\n'Project ['song_id, 'title, 'artist_id, 'year, 'duration]\n+- Project [artist#136, auth#137, firstName#138, gender#139, itemInSession#140L, lastName#141, length#142, level#143, location#144, method#145, page#146, registration#147, sessionId#148L, song#149, status#150L, ts#151L, userAgent#152, userId#153, <lambda>(ts#151L) AS start_time#189]\n   +- Filter (page#146 = NextSong)\n      +- Relation[artist#136,auth#137,firstName#138,gender#139,itemInSession#140L,lastName#141,length#142,level#143,location#144,method#145,page#146,registration#147,sessionId#148L,song#149,status#150L,ts#151L,userAgent#152,userId#153] json\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-a3b5ee7a98f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# extract columns to create songs table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msongs_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"song_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"title\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"artist_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"year\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"duration\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'extract columns to create songs table runtime (s):'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m         \"\"\"\n\u001b[1;32m-> 1421\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`song_id`' given input columns: [artist, auth, firstName, gender, itemInSession, lastName, length, level, location, method, page, registration, sessionId, song, start_time, status, ts, userAgent, userId];;\n'Project ['song_id, 'title, 'artist_id, 'year, 'duration]\n+- Project [artist#136, auth#137, firstName#138, gender#139, itemInSession#140L, lastName#141, length#142, level#143, location#144, method#145, page#146, registration#147, sessionId#148L, song#149, status#150L, ts#151L, userAgent#152, userId#153, <lambda>(ts#151L) AS start_time#189]\n   +- Filter (page#146 = NextSong)\n      +- Relation[artist#136,auth#137,firstName#138,gender#139,itemInSession#140L,lastName#141,length#142,level#143,location#144,method#145,page#146,registration#147,sessionId#148L,song#149,status#150L,ts#151L,userAgent#152,userId#153] json\n"
     ]
    }
   ],
   "source": [
    "# extract columns to create songs table\n",
    "start = time.time()\n",
    "songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\").distinct()\n",
    "end = time.time()\n",
    "print('extract columns to create songs table runtime (s):', end-start)\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "start = time.time()\n",
    "songs_table_parquet = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\", col(\"artist_name\").alias(\"artist\")).distinct()\n",
    "#songs_table_parquet.write.mode('overwrite').partitionBy(\"year\", \"artist\").parquet(output_data + \"songs/\") # S3 location\n",
    "songs_table_parquet.write.mode('overwrite').partitionBy(\"year\", \"artist\").parquet('output_data/' + \"songs/\") # local location\n",
    "end = time.time()\n",
    "print('write songs table to parquet files runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "extract columns to create artists table runtime (s): 0.09255576133728027\nwrite artists table to parquet files runtime (s): 444.856107711792\n"
    }
   ],
   "source": [
    "# extract columns to create artists table\n",
    "start = time.time()\n",
    "artists_table = df.select(\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\").distinct()\n",
    "end = time.time()\n",
    "print('extract columns to create artists table runtime (s):', end-start)\n",
    "\n",
    "# write artists table to parquet files\n",
    "start = time.time()\n",
    "artists_table_parquet = artists_table.write.mode('overwrite').parquet(output_data + \"artists/\")\n",
    "end = time.time()\n",
    "print('write artists table to parquet files runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put finalized code from above in sections below, then transfer to .py file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    '''\n",
    "    This function reads the data from S3, processes the song data using Spark, and writes the processed data back to S3.\n",
    "\n",
    "    Parameters:\n",
    "    - spark: The spark session\n",
    "    - input_data: The S3 path location up to, but not including `song_data`\n",
    "    - output_data: The S3 bucket where the new dimensional tables will be written to\n",
    "    '''\n",
    "    # get filepath to song data file\n",
    "    start = time.time()\n",
    "    song_data =  input_data + 'song_data/*/*/*/*.json'\n",
    "    end = time.time()\n",
    "    print('runtime (s):', end-start)\n",
    "    \n",
    "    # read song data file\n",
    "    start = time.time()\n",
    "    df = spark.read.json(song_data) # may not need this new schema\n",
    "    end = time.time()\n",
    "    print('read song data file runtime (s):', end-start)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    start = time.time()\n",
    "    songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\").distinct()\n",
    "    end = time.time()\n",
    "    print('extract columns to create songs table runtime (s):', end-start)\n",
    "\n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    start = time.time()\n",
    "    songs_table_parquet = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\", col(\"artist_name\").alias(\"artist\")).distinct()\n",
    "    songs_table_parquet.write.mode('overwrite').partitionBy(\"year\", \"artist_id\").parquet(output_data + \"songs/\")\n",
    "    end = time.time()\n",
    "    print('write songs table to parquet files runtime (s):', end-start)\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    start = time.time()\n",
    "    artists_table = df.select(\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\").distinct()\n",
    "    end = time.time()\n",
    "    print('extract columns to create artists table runtime (s):', end-start)\n",
    "\n",
    "    # write artists table to parquet files\n",
    "    start = time.time()\n",
    "    artists_table_parquet = artists_table.write.mode('overwrite').parquet(output_data + \"artists/\")\n",
    "    end = time.time()\n",
    "    print('write artists table to parquet files runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "   '''\n",
    "    This function reads the data from S3, processes the log data using Spark,\n",
    "    and writes the processed data back to S3.\n",
    "   '''\n",
    "    # get filepath to log data file\n",
    "    start = time.time()\n",
    "    log_data = input_data + 'log_data/*/*/*.json'\n",
    "    end = time.time()\n",
    "    print('get log filepath runtime (s):', end-start)\n",
    "\n",
    "    # read log data file\n",
    "    start = time.time()\n",
    "    df = spark.read.json(log_data)\n",
    "    end = time.time()\n",
    "    print('read log data file runtime (s):', end-start)\n",
    "\n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(df.page=='NextSong')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    start = time.time()\n",
    "    users_table = df.select(\"user_id\", \"first_name\", \"last_name\", \"gender\", \"level\").distinct()\n",
    "    end = time.time()\n",
    "    print('extract columns for users table runtime (s):', end-start)\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    start = time.time()\n",
    "    users_table.write.mode('overwrite').partitionBy(\"year\", \"artist_id\").parquet(output_data + \"users/\")\n",
    "    end = time.time()\n",
    "    print('write users table to parquet files runtime (s):', end-start)\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = F.udf(lambda x: datetime.fromtimestamp(x/1000.0), T.TimestampType() )\n",
    "    df = df.withColumn(\"timestamp\", get_timestamp(df.ts)) # creates new column named timestamp, populates it with converted timestamp\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = F.udf(lambda x: datetime.fromtimestamp(x/1000.0) )\n",
    "    df = df.withColumn(\"datetime\", get_timestamp(df.ts))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.select(datetime)\n",
    "    time_table = time_table.withColumn(\"month\", F.month(\"datetime\"), )\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.mode('overwrite').partitionBy(\"year\", \"month\").parquet(output_data + \"time/\")\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_df = \n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = \n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    Create Spark session, provide paths to input/output data, load songs/log data\n",
    "    and create parquet tables (columnar format) with star schema DB.\n",
    "    '''\n",
    "    spark = create_spark_session()\n",
    "    input_data = \"s3a://udacity-dend/\"\n",
    "    output_data = \"s3a://dend-sparkify-amiri/output_data\"\n",
    "    \n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANDBOX AREA BELOW ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "get log filepath runtime (s): 0.0009980201721191406\n"
    }
   ],
   "source": [
    "# get filepath to log data file\n",
    "start = time.time()\n",
    "log_data = input_data + 'log_data/*/*/*.json'\n",
    "end = time.time()\n",
    "print('get log filepath runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "read log data file runtime (s): 10.974141597747803\n"
    }
   ],
   "source": [
    "# read log data file\n",
    "start = time.time()\n",
    "df = spark.read.json(log_data)\n",
    "end = time.time()\n",
    "print('read log data file runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: double, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "extract columns for users table runtime (s): 0.0638277530670166\n"
    }
   ],
   "source": [
    "# filter by actions for song plays\n",
    "df = df.filter(df.page=='NextSong')\n",
    "\n",
    "# extract columns for users table    \n",
    "start = time.time()\n",
    "users_table = df.select(\"userid\", \"firstName\", \"lastName\", \"gender\", \"level\").distinct()\n",
    "end = time.time()\n",
    "print('extract columns for users table runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "write users table to parquet files runtime (s): 55.40958642959595\n"
    }
   ],
   "source": [
    "# write users table to parquet files\n",
    "start = time.time()\n",
    "users_table.write.mode('overwrite').parquet(output_data + \"users/\")\n",
    "end = time.time()\n",
    "print('write users table to parquet files runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = F.udf(lambda x: datetime.fromtimestamp(x/1000.0), T.TimestampType() )\n",
    "df = df.withColumn(\"timestamp\", get_timestamp(df.ts)) # creates new column named timestamp, populates it with converted timestamp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetime column from original timestamp column\n",
    "get_datetime = F.udf(lambda x: datetime.fromtimestamp(x/1000.0), T.TimestampType() )\n",
    "df = df.withColumn(\"start_time\", get_datetime(df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "time_table = df.select(\"start_time\").distinct().withColumn(\"hour\",    F.hour(df.start_time)) \\\n",
    "                                               .withColumn(\"day\",     F.dayofmonth(df.start_time)) \\\n",
    "                                               .withColumn(\"week\",    F.weekofyear(df.start_time)) \\\n",
    "                                               .withColumn(\"month\",   F.month(df.start_time)) \\\n",
    "                                               .withColumn(\"year\",    F.year(df.start_time)) \\\n",
    "                                               .withColumn(\"weekday\", F.date_format(df.start_time, \"E\")) # the 'E' formats this parameter to day-of-the-week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[start_time: timestamp, hour: int, day: int, week: int, month: int, year: int, weekday: string]"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "time_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table.write.mode('overwrite').parquet(output_data + \"users/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.mode('overwrite').partitionBy(\"year\", \"month\").parquet(output_data + \"time/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "song_df_path = 'output_data/' + 'songs/*/*/*'\n",
    "song_df = spark.read.parquet(song_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pyspark.sql.dataframe.DataFrame'>\n+------------------+--------------------+----------+--------------------+\n|           song_id|               title|  duration|              artist|\n+------------------+--------------------+----------+--------------------+\n|SOEPTVC12A67ADD0DA|To Zucchabar [\"Gl...| 196.04853|Yvonne S. Moriart...|\n|SOVPFJK12A6701CB16|Barcelona - (Frie...| 273.44934|Russell Watson / ...|\n|SOQEBML12A8C136AA4|Werther (1997 Dig...|  55.40526|Alfredo Kraus/Tat...|\n|SODUMDU12AC468A22B|We're Skrewed (Ot...|  249.5473|         All Leather|\n|SOPSXLI12A6D4FA418|Practical Cats - ...| 251.03628|Robert Donat/Phil...|\n|SOTCIHX12A8C13DDD2|Finally_ as that ...| 483.34322|        Red Sparowes|\n|SOKUATC12AB01853F3|Turandot: Nessun ...| 191.13751|Andrea Bocelli / ...|\n|SOUFBFK12A8C13D668|String Quartets O...| 348.60363|Endellion String ...|\n|SONVWOX12A8C137FF5|Le Roi de Lahore_...| 250.48771|Rolando Villazon/...|\n|SOUQKQA12A6D4F951E|Perfume: The Stor...| 331.20608|Berliner Philharm...|\n|SOHPSTY12A6D4F719C|Ms. New Booty (Ed...| 252.52526|Bubba Sparxxx fea...|\n|SOMDBLQ12A6D4F8B72|Quartet for Piano...| 496.53506|Martha Argerich/R...|\n|SOIEUCF12A6D4F9889|Symphony in B fla...| 398.34077|Paul Hindemith/Ph...|\n|SOIGELY12A6D4F65C6|Aida (1986 Digita...| 345.25995|Agnes Baltsa/Mire...|\n|SOBTCUI12A8AE48B70|Faust: Ballet Mus...|  94.56281|Royal Philharmoni...|\n|SOSZXLP12A8C133EE6|Three Botticelli ...| 282.74893|Sir Neville Marri...|\n|SOROAMT12A8C13C6D0|Me gustan mas los...| 101.85098|          Albertucho|\n|SOLLALT12A8C1399F3|Piano Concerto No...| 319.37261|Dmitri Shostakovi...|\n|SOQYHYG12AB017CF0B|The Horrors Of Is...| 279.95383|    The Flaming Lips|\n|SOAZUNV12A8C13923B|A Faust Symphony ...|1237.86404|Royal Philharmoni...|\n+------------------+--------------------+----------+--------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "print(type(song_df))\n",
    "song_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = spark.sql(\"SELECT s.song_id as songplay_id , l.start_time, l.user_id, l.level, s.song_id, l.artist_id, l.session_id, l.location, l.user_agent  \\\n",
    "                              FROM ( \\\n",
    "                                  SELECT DISTINCT song_df \\\n",
    "                                  FROM song_df \\\n",
    "                              JOIN ( \\\n",
    "                                  )\n",
    "                                  )\n",
    "    \n",
    "    \n",
    "    \n",
    "                           \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`L.artistId`' given input columns: [S.artist, L.artist, L.auth, S.duration, L.firstName, L.gender, L.itemInSession, L.lastName, L.length, L.level, L.location, L.method, L.page, L.registration, L.sessionId, L.song, S.song_id, L.start_time, L.status, S.title, L.ts, L.userAgent, L.userId]; line 1 pos 51;\n'Project [start_time#189, userId#153, level#143, song_id#16, 'L.artistId, 'LsessionId, location#144, userAgent#152]\n+- Join Inner, (artist#136 = artist#19)\n   :- SubqueryAlias L\n   :  +- SubqueryAlias log_dataset\n   :     +- Project [artist#136, auth#137, firstName#138, gender#139, itemInSession#140L, lastName#141, length#142, level#143, location#144, method#145, page#146, registration#147, sessionId#148L, song#149, status#150L, ts#151L, userAgent#152, userId#153, <lambda>(ts#151L) AS start_time#189]\n   :        +- Filter (page#146 = NextSong)\n   :           +- Relation[artist#136,auth#137,firstName#138,gender#139,itemInSession#140L,lastName#141,length#142,level#143,location#144,method#145,page#146,registration#147,sessionId#148L,song#149,status#150L,ts#151L,userAgent#152,userId#153] json\n   +- SubqueryAlias S\n      +- SubqueryAlias song_dataset\n         +- Relation[song_id#16,title#17,duration#18,artist#19] parquet\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-ee3128166fd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msong_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"song_dataset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"log_dataset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m song_df2 = spark.sql(\"SELECT L.start_time, L.userId, L.level, S.song_id, L.artistId, LsessionId, L.location, L.userAgent \\\n\u001b[0m\u001b[0;32m      4\u001b[0m                         \u001b[0mFROM\u001b[0m \u001b[0mlog_dataset\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mL\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                           \u001b[0mJOIN\u001b[0m \u001b[0msong_dataset\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mS\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \"\"\"\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '`L.artistId`' given input columns: [S.artist, L.artist, L.auth, S.duration, L.firstName, L.gender, L.itemInSession, L.lastName, L.length, L.level, L.location, L.method, L.page, L.registration, L.sessionId, L.song, S.song_id, L.start_time, L.status, S.title, L.ts, L.userAgent, L.userId]; line 1 pos 51;\n'Project [start_time#189, userId#153, level#143, song_id#16, 'L.artistId, 'LsessionId, location#144, userAgent#152]\n+- Join Inner, (artist#136 = artist#19)\n   :- SubqueryAlias L\n   :  +- SubqueryAlias log_dataset\n   :     +- Project [artist#136, auth#137, firstName#138, gender#139, itemInSession#140L, lastName#141, length#142, level#143, location#144, method#145, page#146, registration#147, sessionId#148L, song#149, status#150L, ts#151L, userAgent#152, userId#153, <lambda>(ts#151L) AS start_time#189]\n   :        +- Filter (page#146 = NextSong)\n   :           +- Relation[artist#136,auth#137,firstName#138,gender#139,itemInSession#140L,lastName#141,length#142,level#143,location#144,method#145,page#146,registration#147,sessionId#148L,song#149,status#150L,ts#151L,userAgent#152,userId#153] json\n   +- SubqueryAlias S\n      +- SubqueryAlias song_dataset\n         +- Relation[song_id#16,title#17,duration#18,artist#19] parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "song_df.createOrReplaceTempView(\"song_dataset\")\n",
    "df.createOrReplaceTempView(\"log_dataset\")\n",
    "song_df2 = spark.sql(\"SELECT L.start_time, L.userId, L.level, S.song_id, L.artistId, LsessionId, L.location, L.userAgent \\\n",
    "                        FROM log_dataset as L \\\n",
    "                          JOIN song_dataset as S  \\\n",
    "                            ON L.artist = S.artist \" )\n",
    "song_df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AnalysisException",
     "evalue": "Table or view not found: df; line 1 pos 44;\n'Project ['L.start_time, 'L.userId, 'L.level]\n+- 'SubqueryAlias L\n   +- 'UnresolvedRelation [df]\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-f667edee8c52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msong_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"song_dataset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"log_dataset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msong_df2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT L.start_time, L.userId, L.level FROM df as L\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \"\"\"\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Table or view not found: df; line 1 pos 44;\n'Project ['L.start_time, 'L.userId, 'L.level]\n+- 'SubqueryAlias L\n   +- 'UnresolvedRelation [df]\n"
     ]
    }
   ],
   "source": [
    "song_df.createOrReplaceTempView(\"song_dataset\")\n",
    "df.createOrReplaceTempView(\"log_dataset\")\n",
    "song_df2 = spark.sql(\"SELECT L.start_time, L.userId, L.level FROM df as L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: double, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string, start_time: timestamp]"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'log_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-d3f20472b67d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlog_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'log_data' is not defined"
     ]
    }
   ],
   "source": [
    "log_df = spark.read.json(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pyspark.sql.dataframe.DataFrame'>\n"
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# This code is from the internet, and it works\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local')\n",
    "create_spark_session()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works! Stops the spark context\n",
    "SparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install boto3\n",
    "# Also had to install Java SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to udacity bucket with the console:  https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/?region=us-west-2\n",
    "#s3path = 'https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/?region=us-west-2#'\n",
    "#s3pathsong = 'https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/song_data/A/?region=us-west-2'\n"
   ]
  }
 ]
}