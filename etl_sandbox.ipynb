{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38332bit92b6213f5c2744d0ab77ccfa3a973704",
   "display_name": "Python 3.8.3 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nIn order to properly setup my local environment:\\n- pip install boto3\\n- I had to install Spark, Java, Hadoop, and SBT on my Windows 10 machine, ADD YT LINK FOR SBT TOO!!\\nfollowing the instructions at this link: https://www.youtube.com/watch?v=g7Qpnmi0Q-s for the Java/Hadoop portion.\\n'"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "'''\n",
    "In order to properly setup my local environment:\n",
    "- pip install boto3\n",
    "- I had to install Spark, Java, Hadoop, and SBT on my Windows 10 machine, ADD YT LINK FOR SBT TOO!!\n",
    "following the instructions at this link: https://www.youtube.com/watch?v=g7Qpnmi0Q-s for the Java/Hadoop portion.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import types as T\n",
    "import  pyspark.sql.functions as F\n",
    "import time\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['dl.cfg']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0xfd482b0>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://DESKTOP-GJKKHSO:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to speed up parquet write\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set(\"spark.speculation\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "runtime (s): 492.0592863559723\n"
    }
   ],
   "source": [
    "# Not sure that second line is faster than first\n",
    "#df = spark.read.format(\"json\").load(\"s3a://udacity-dend/song_data/*/*/*\") # runs in 13 minutes\n",
    "#df = spark.read.format(\"json\").load(file_locations)\n",
    "# df = spark.read.format(\"json\").load(songs_paths)\n",
    "# song_data = input_data+'song_data/*/*/*/*.json\n",
    "# song_data = input_data + 'song_data/*/*/*/*.json # runs in 14 minutes\n",
    "# df = spark.read.json(song_data) # runs in 14 minutes\n",
    "# df = spark.read.json(songs_paths) # runs in 26 minutes\n",
    "\n",
    "start = time.time()\n",
    "#input_data = \"s3a://udacity-dend/\"\n",
    "#song_data =  input_data + 'song_data/*/*/*/*.json'\n",
    "df_song_data = spark.read.json(\"s3a://udacity-dend/song_data/*/*/*\") # runs in 13 minutes\n",
    "end = time.time()\n",
    "print('runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"\\nstart = time.time()\\nprint(type(df))\\nprint(df)\\nprint('count', df.count())\\n#df.printSchema()\\nend = time.time()\\nprint('runtime (s):', end-start)\\n\""
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "'''\n",
    "start = time.time()\n",
    "print(type(df))\n",
    "print(df)\n",
    "print('count', df.count())\n",
    "#df.printSchema()\n",
    "end = time.time()\n",
    "print('runtime (s):', end-start)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://dend-sparkify-amiri/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "extract columns to create songs table runtime (s): 0.03291201591491699\nwrite songs table to parquet files runtime (s): 389.07194328308105\n"
    }
   ],
   "source": [
    "# extract columns to create songs table\n",
    "start = time.time()\n",
    "songs_table = df_song_data.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\").distinct()\n",
    "end = time.time()\n",
    "print('extract columns to create songs table runtime (s):', end-start)\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist (tried using alias for artist_name, partitionBy does not seem to like this)\n",
    "start = time.time()\n",
    "songs_table_parquet = df_song_data.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\", \"artist_name\").distinct()\n",
    "#songs_table_parquet.write.mode('overwrite').partitionBy(\"year\", \"artist\").parquet(output_data + \"songs/\") # S3 location\n",
    "songs_table_parquet.write.mode('overwrite').partitionBy(\"year\", \"artist_id\").parquet('output_data/' + \"songs/\") # local location\n",
    "end = time.time()\n",
    "print('write songs table to parquet files runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "extract columns to create artists table runtime (s): 0.029893875122070312\nwrite artists table to parquet files runtime (s): 409.5495846271515\n"
    }
   ],
   "source": [
    "# extract columns to create artists table\n",
    "start = time.time()\n",
    "artists_table = df_song_data.select(\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\").distinct()\n",
    "end = time.time()\n",
    "print('extract columns to create artists table runtime (s):', end-start)\n",
    "\n",
    "# write artists table to parquet files\n",
    "start = time.time()\n",
    "artists_table_parquet = artists_table.write.mode('overwrite').parquet(output_data + \"artists/\")\n",
    "end = time.time()\n",
    "print('write artists table to parquet files runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put finalized code from above in sections below, then transfer to .py file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    '''\n",
    "    This function reads the data from S3, processes the song data using Spark, and writes the processed data back to S3.\n",
    "\n",
    "    Parameters:\n",
    "    - spark: The spark session\n",
    "    - input_data: The S3 path location up to, but not including `song_data`\n",
    "    - output_data: The S3 bucket where the new dimensional tables will be written to\n",
    "    '''\n",
    "    # get filepath to song data file\n",
    "    start = time.time()\n",
    "    song_data =  input_data + 'song_data/*/*/*/*.json'\n",
    "    end = time.time()\n",
    "    print('runtime (s):', end-start)\n",
    "    \n",
    "    # read song data file\n",
    "    start = time.time()\n",
    "    df = spark.read.json(song_data) # may not need this new schema\n",
    "    end = time.time()\n",
    "    print('read song data file runtime (s):', end-start)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    start = time.time()\n",
    "    songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\").distinct()\n",
    "    end = time.time()\n",
    "    print('extract columns to create songs table runtime (s):', end-start)\n",
    "\n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    start = time.time()\n",
    "    songs_table_parquet = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\", col(\"artist_name\").alias(\"artist\")).distinct()\n",
    "    songs_table_parquet.write.mode('overwrite').partitionBy(\"year\", \"artist_id\").parquet(output_data + \"songs/\")\n",
    "    end = time.time()\n",
    "    print('write songs table to parquet files runtime (s):', end-start)\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    start = time.time()\n",
    "    artists_table = df.select(\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\").distinct()\n",
    "    end = time.time()\n",
    "    print('extract columns to create artists table runtime (s):', end-start)\n",
    "\n",
    "    # write artists table to parquet files\n",
    "    start = time.time()\n",
    "    artists_table_parquet = artists_table.write.mode('overwrite').parquet(output_data + \"artists/\")\n",
    "    end = time.time()\n",
    "    print('write artists table to parquet files runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "   '''\n",
    "    This function reads the data from S3, processes the log data using Spark,\n",
    "    and writes the processed data back to S3.\n",
    "   '''\n",
    "    # get filepath to log data file\n",
    "    start = time.time()\n",
    "    log_data = input_data + 'log_data/*/*/*.json'\n",
    "    end = time.time()\n",
    "    print('get log filepath runtime (s):', end-start)\n",
    "\n",
    "    # read log data file\n",
    "    start = time.time()\n",
    "    df = spark.read.json(log_data)\n",
    "    end = time.time()\n",
    "    print('read log data file runtime (s):', end-start)\n",
    "\n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(df.page=='NextSong')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    start = time.time()\n",
    "    users_table = df.select(\"user_id\", \"first_name\", \"last_name\", \"gender\", \"level\").distinct()\n",
    "    end = time.time()\n",
    "    print('extract columns for users table runtime (s):', end-start)\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    start = time.time()\n",
    "    users_table.write.mode('overwrite').partitionBy(\"year\", \"artist_id\").parquet(output_data + \"users/\")\n",
    "    end = time.time()\n",
    "    print('write users table to parquet files runtime (s):', end-start)\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = F.udf(lambda x: datetime.fromtimestamp(x/1000.0), T.TimestampType() )\n",
    "    df = df.withColumn(\"timestamp\", get_timestamp(df.ts)) # creates new column named timestamp, populates it with converted timestamp\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = F.udf(lambda x: datetime.fromtimestamp(x/1000.0) )\n",
    "    df = df.withColumn(\"datetime\", get_timestamp(df.ts))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = df.select(datetime)\n",
    "    time_table = time_table.withColumn(\"month\", F.month(\"datetime\"), )\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.mode('overwrite').partitionBy(\"year\", \"month\").parquet(output_data + \"time/\")\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_df = \n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = \n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    Create Spark session, provide paths to input/output data, load songs/log data\n",
    "    and create parquet tables (columnar format) with star schema DB.\n",
    "    '''\n",
    "    spark = create_spark_session()\n",
    "    input_data = \"s3a://udacity-dend/\"\n",
    "    output_data = \"s3a://dend-sparkify-amiri/output_data\"\n",
    "    \n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANDBOX AREA BELOW ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "get log filepath runtime (s): 0.0\n"
    }
   ],
   "source": [
    "# get filepath to log data file\n",
    "start = time.time()\n",
    "log_data = input_data + 'log_data/*/*/*.json'\n",
    "end = time.time()\n",
    "print('get log filepath runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "read log data file runtime (s): 8.51508903503418\n"
    }
   ],
   "source": [
    "# read log data file\n",
    "start = time.time()\n",
    "df_log_data = spark.read.json(log_data)\n",
    "end = time.time()\n",
    "print('read log data file runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: double, sessionId: bigint, song: string, status: bigint, ts: bigint, userAgent: string, userId: string]"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df_log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "extract columns for users table runtime (s): 0.017918825149536133\n"
    }
   ],
   "source": [
    "# filter by actions for song plays\n",
    "df_log_data = df_log_data.filter(df_log_data.page=='NextSong')\n",
    "\n",
    "# extract columns for users table    \n",
    "start = time.time()\n",
    "users_table = df_log_data.select(\"userid\", \"firstName\", \"lastName\", \"gender\", \"level\").distinct()\n",
    "end = time.time()\n",
    "print('extract columns for users table runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "write users table to parquet files runtime (s): 53.5797758102417\n"
    }
   ],
   "source": [
    "# write users table to parquet files\n",
    "start = time.time()\n",
    "users_table.write.mode('overwrite').parquet(output_data + \"users/\")\n",
    "end = time.time()\n",
    "print('write users table to parquet files runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = F.udf(lambda x: datetime.fromtimestamp(x/1000.0), T.TimestampType() )\n",
    "df = df.withColumn(\"timestamp\", get_timestamp(df.ts)) # creates new column named timestamp, populates it with converted timestamp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetime column from original timestamp column\n",
    "get_datetime = F.udf(lambda x: datetime.fromtimestamp(x/1000.0), T.TimestampType() )\n",
    "df_log_data = df_log_data.withColumn(\"start_time\", get_datetime(df_log_data.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write users table to parquet files\n",
    "users_table.write.mode('overwrite').parquet(output_data + \"users/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "time_table = df_log_data.select(\"start_time\").distinct().withColumn(\"hour\",    F.hour(df_log_data.start_time)) \\\n",
    "                                                        .withColumn(\"day\",     F.dayofmonth(df_log_data.start_time)) \\\n",
    "                                                        .withColumn(\"week\",    F.weekofyear(df_log_data.start_time)) \\\n",
    "                                                        .withColumn(\"month\",   F.month(df_log_data.start_time)) \\\n",
    "                                                        .withColumn(\"year\",    F.year(df_log_data.start_time)) \\\n",
    "                                                        .withColumn(\"weekday\", F.date_format(df_log_data.start_time, \"E\")) # the 'E' formats this parameter to day-of-the-week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+--------------------+----+---+----+-----+----+-------+\n|          start_time|hour|day|week|month|year|weekday|\n+--------------------+----+---+----+-----+----+-------+\n|2018-11-20 23:18:...|  23| 20|  47|   11|2018|    Tue|\n|2018-11-21 11:49:...|  11| 21|  47|   11|2018|    Wed|\n|2018-11-14 08:20:...|   8| 14|  46|   11|2018|    Wed|\n+--------------------+----+---+----+-----+----+-------+\nonly showing top 3 rows\n\n"
    }
   ],
   "source": [
    "time_table.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.mode('overwrite').partitionBy(\"year\", \"month\").parquet(output_data + \"time/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "song_df_path = 'output_data/' + 'songs/*/*/*'\n",
    "#song_df_path = r'C:\\Users\\amiri\\Documents\\GitHub\\Data_Engineering_Data_Lake_with_Spark\\output_data'\n",
    "#song_df = spark.read.parquetoutput_data + 'songs/*/*/*')\n",
    "song_df = spark.read.parquet(song_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pyspark.sql.dataframe.DataFrame'>\n+------------------+--------------------+---------+--------------------+\n|           song_id|               title| duration|         artist_name|\n+------------------+--------------------+---------+--------------------+\n|SOEPTVC12A67ADD0DA|To Zucchabar [\"Gl...|196.04853|Yvonne S. Moriart...|\n|SOVPFJK12A6701CB16|Barcelona - (Frie...|273.44934|Russell Watson / ...|\n|SOQEBML12A8C136AA4|Werther (1997 Dig...| 55.40526|Alfredo Kraus/Tat...|\n+------------------+--------------------+---------+--------------------+\nonly showing top 3 rows\n\n"
    }
   ],
   "source": [
    "print(type(song_df))\n",
    "song_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pyspark.sql.dataframe.DataFrame'>\n+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+--------------------+\n|     artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|           song|status|           ts|           userAgent|userId|          start_time|\n+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+--------------------+\n|   Harmonia|Logged In|     Ryan|     M|            0|   Smith|655.77751| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|  Sehr kosmisch|   200|1542241826796|\"Mozilla/5.0 (X11...|    26|2018-11-14 17:30:...|\n|The Prodigy|Logged In|     Ryan|     M|            1|   Smith|260.07465| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|The Big Gundown|   200|1542242481796|\"Mozilla/5.0 (X11...|    26|2018-11-14 17:41:...|\n|      Train|Logged In|     Ryan|     M|            2|   Smith|205.45261| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|       Marry Me|   200|1542242741796|\"Mozilla/5.0 (X11...|    26|2018-11-14 17:45:...|\n+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+--------------------+\nonly showing top 3 rows\n\n"
    }
   ],
   "source": [
    "print(type(df_log_data))\n",
    "df_log_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- song_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- duration: double (nullable = true)\n |-- artist_name: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "song_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- artist: string (nullable = true)\n |-- auth: string (nullable = true)\n |-- firstName: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- itemInSession: long (nullable = true)\n |-- lastName: string (nullable = true)\n |-- length: double (nullable = true)\n |-- level: string (nullable = true)\n |-- location: string (nullable = true)\n |-- method: string (nullable = true)\n |-- page: string (nullable = true)\n |-- registration: double (nullable = true)\n |-- sessionId: long (nullable = true)\n |-- song: string (nullable = true)\n |-- status: long (nullable = true)\n |-- ts: long (nullable = true)\n |-- userAgent: string (nullable = true)\n |-- userId: string (nullable = true)\n |-- start_time: timestamp (nullable = true)\n\n"
    }
   ],
   "source": [
    "#df_log_data.schema.names\n",
    "df_log_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- artist_id: string (nullable = true)\n |-- artist_name: string (nullable = true)\n |-- artist_location: string (nullable = true)\n |-- artist_latitude: double (nullable = true)\n |-- artist_longitude: double (nullable = true)\n\n"
    }
   ],
   "source": [
    "#df_log_data.schema.names\n",
    "artists_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets to create songplays table \n",
    "# songplays table columns: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "# need to get artist_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in artist data so that i have access to artist_id column\n",
    "artists_df = spark.read.parquet(output_data + 'artists/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join songs are log data on title column\n",
    "songs_logs_df = df_log_data.join(song_df, df_log_data.song == song_df.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_artists_df = songs_logs_df.join(artists_df, songs_logs_df.artist_name == artists_df.artist_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- artist: string (nullable = true)\n |-- auth: string (nullable = true)\n |-- firstName: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- itemInSession: long (nullable = true)\n |-- lastName: string (nullable = true)\n |-- length: double (nullable = true)\n |-- level: string (nullable = true)\n |-- location: string (nullable = true)\n |-- method: string (nullable = true)\n |-- page: string (nullable = true)\n |-- registration: double (nullable = true)\n |-- sessionId: long (nullable = true)\n |-- song: string (nullable = true)\n |-- status: long (nullable = true)\n |-- ts: long (nullable = true)\n |-- userAgent: string (nullable = true)\n |-- userId: string (nullable = true)\n |-- start_time: timestamp (nullable = true)\n |-- song_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- duration: double (nullable = true)\n |-- artist_name: string (nullable = true)\n |-- artist_id: string (nullable = true)\n |-- artist_name: string (nullable = true)\n |-- artist_location: string (nullable = true)\n |-- artist_latitude: double (nullable = true)\n |-- artist_longitude: double (nullable = true)\n\n"
    }
   ],
   "source": [
    "songs_artists_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songplays table columns: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "songplays_table = songs_artists_df.select(\"start_time\", \"userId\", \"level\", \"song_id\", \"artist_id\", \"sessionId\", \"location\", \"userAgent\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----------+--------------------+------+-----+------------------+------------------+---------+--------------------+--------------------+----+-----+\n|songplay_id|          start_time|userId|level|           song_id|         artist_id|sessionId|            location|           userAgent|year|month|\n+-----------+--------------------+------+-----+------------------+------------------+---------+--------------------+--------------------+----+-----+\n|          0|2018-11-29 09:58:...|    49| paid|SOGXSWA12A6D4FBC99|ARPFHN61187FB575F6|     1041|San Francisco-Oak...|Mozilla/5.0 (Wind...|2018|   11|\n|          1|2018-11-19 18:57:...|    85| paid|SORZRUJ12A6D4F61A6|ARWVYIM11F4C8411E4|      658|       Red Bluff, CA|\"Mozilla/5.0 (Mac...|2018|   11|\n|          2|2018-11-16 12:22:...|    49| paid|SOAHNFK12AC9075097|ARZIFFT1187FB41B2A|      648|San Francisco-Oak...|Mozilla/5.0 (Wind...|2018|   11|\n+-----------+--------------------+------+-----+------------------+------------------+---------+--------------------+--------------------+----+-----+\nonly showing top 3 rows\n\n"
    }
   ],
   "source": [
    "songplays_table = songplays_table.withColumn(\"songplay_id\", monotonically_increasing_id()) # create songplay_id and make it primary key\n",
    "songplays_table = songplays_table.select(\"songplay_id\", \"start_time\", \"userId\", \"level\", \"song_id\", \"artist_id\", \"sessionId\", \"location\", \"userAgent\") # rearrange\n",
    "songplays_table = songplays_table.withColumn(\"year\", F.year(\"start_time\")).withColumn(\"month\", F.month(\"start_time\"))\n",
    "songplays_table.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.write.mode('overwrite').partitionBy(\"year\", \"month\").parquet(output_data + \"songplays/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# This code is from the internet, and it works\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local')\n",
    "create_spark_session()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works! Stops the spark context\n",
    "SparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install boto3\n",
    "# Also had to install Java SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to udacity bucket with the console:  https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/?region=us-west-2\n",
    "#s3path = 'https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/?region=us-west-2#'\n",
    "#s3pathsong = 'https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/song_data/A/?region=us-west-2'\n"
   ]
  }
 ]
}