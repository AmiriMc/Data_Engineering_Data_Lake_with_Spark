{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38332bit92b6213f5c2744d0ab77ccfa3a973704",
   "display_name": "Python 3.8.3 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In order to properly setup my local environment:\n",
    "- pip install boto3\n",
    "- I had to install Spark, Java and Hadoop on my Windows 10 machine,\n",
    "following the instructions at this link: https://www.youtube.com/watch?v=g7Qpnmi0Q-s for the Java/Hadoop portion.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql import types as T\n",
    "import  pyspark.sql.functions as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['dl.cfg']"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0xf364f58>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://DESKTOP-GJKKHSO:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to speed up parquet write\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "runtime (s): 950.199615240097\n"
    }
   ],
   "source": [
    "# Not sure that second line is faster than first\n",
    "#df = spark.read.format(\"json\").load(\"s3a://udacity-dend/song_data/*/*/*\") # runs in 13 minutes\n",
    "#df = spark.read.format(\"json\").load(file_locations)\n",
    "# df = spark.read.format(\"json\").load(songs_paths)\n",
    "# song_data = input_data+'song_data/*/*/*/*.json\n",
    "# song_data = input_data + 'song_data/*/*/*/*.json # runs in 14 minutes\n",
    "# df = spark.read.json(song_data) # runs in 14 minutes\n",
    "# df = spark.read.json(songs_paths) # runs in 26 minutes\n",
    "\n",
    "start = time.time()\n",
    "input_data = \"s3a://udacity-dend/\"\n",
    "song_data =  input_data + 'song_data/*/*/*/*.json'\n",
    "df = spark.read.format(\"json\").load(\"s3a://udacity-dend/song_data/*/*/*\") # runs in 13 minutes\n",
    "end = time.time()\n",
    "print('runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pyspark.sql.dataframe.DataFrame'>\nDataFrame[artist_id: string, artist_latitude: double, artist_location: string, artist_longitude: double, artist_name: string, duration: double, num_songs: bigint, song_id: string, title: string, year: bigint]\ncount 14896\nruntime (s): 339.83151745796204\n"
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(type(df))\n",
    "print(df)\n",
    "print('count', df.count())\n",
    "#df.printSchema()\n",
    "end = time.time()\n",
    "print('runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://dend-sparkify-amiri/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "extract columns to create songs table runtime (s): 0.04885745048522949\n"
    }
   ],
   "source": [
    "# extract columns to create songs table\n",
    "start = time.time()\n",
    "songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\").distinct()\n",
    "end = time.time()\n",
    "print('extract columns to create songs table runtime (s):', end-start)\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "start = time.time()\n",
    "songs_table_parquet = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\", col(\"artist_name\").alias(\"artist\")).distinct()\n",
    "songs_table_parquet.write.mode('overwrite').partitionBy(\"year\", \"artist_id\").parquet(output_data + \"songs/\")\n",
    "end = time.time()\n",
    "print('write songs table to parquet files runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "extract columns to create artists table runtime (s): 0.0378720760345459\nwrite artists table to parquet files runtime (s): 330.1275327205658\n"
    }
   ],
   "source": [
    "# extract columns to create artists table\n",
    "start = time.time()\n",
    "artists_table = df.select(\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\").distinct()\n",
    "end = time.time()\n",
    "print('extract columns to create artists table runtime (s):', end-start)\n",
    "\n",
    "# write artists table to parquet files\n",
    "start = time.time()\n",
    "artists_table_parquet = artists_table.write.mode('overwrite').parquet(output_data + \"artists/\")\n",
    "end = time.time()\n",
    "print('write artists table to parquet files runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put finalized code from above in sections below, then transfer to .py file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    '''\n",
    "    This function reads the data from S3, processes the song data using Spark, and writes the processed data back to S3.\n",
    "\n",
    "    Parameters:\n",
    "    - spark: The spark session\n",
    "    - input_data: The S3 path location up to, but not including `song_data`\n",
    "    - output_data: The S3 bucket where the new dimensional tables will be written to\n",
    "    '''\n",
    "    # get filepath to song data file\n",
    "    start = time.time()\n",
    "    song_data =  input_data + 'song_data/*/*/*/*.json'\n",
    "    end = time.time()\n",
    "    print('runtime (s):', end-start)\n",
    "    \n",
    "    # read song data file\n",
    "    start = time.time()\n",
    "    df = spark.read.json(song_data) # may not need this new schema\n",
    "    end = time.time()\n",
    "    print('read song data file runtime (s):', end-start)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    start = time.time()\n",
    "    songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\").distinct()\n",
    "    end = time.time()\n",
    "    print('extract columns to create songs table runtime (s):', end-start)\n",
    "\n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    start = time.time()\n",
    "    songs_table_parquet = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\", col(\"artist_name\").alias(\"artist\")).distinct()\n",
    "    songs_table_parquet.write.mode('overwrite').partitionBy(\"year\", \"artist_id\").parquet(output_data + \"songs/\")\n",
    "    end = time.time()\n",
    "    print('write songs table to parquet files runtime (s):', end-start)\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    start = time.time()\n",
    "    artists_table = df.select(\"artist_id\", \"artist_name\", \"artist_location\", \"artist_latitude\", \"artist_longitude\").distinct()\n",
    "    end = time.time()\n",
    "    print('extract columns to create artists table runtime (s):', end-start)\n",
    "\n",
    "    # write artists table to parquet files\n",
    "    start = time.time()\n",
    "    artists_table_parquet = artists_table.write.mode('overwrite').parquet(output_data + \"artists/\")\n",
    "    end = time.time()\n",
    "    print('write artists table to parquet files runtime (s):', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "   '''\n",
    "    This function reads the data from S3, processes the log data using Spark,\n",
    "    and writes the processed data back to S3.\n",
    "   '''\n",
    "    # get filepath to log data file\n",
    "    start = time.time()\n",
    "    log_data = input_data + 'log_data/*/*/*.json'\n",
    "    end = time.time()\n",
    "    print('get log filepath runtime (s):', end-start)\n",
    "\n",
    "    # read log data file\n",
    "    start = time.time()\n",
    "    df = spark.read.json(log_data)\n",
    "    end = time.time()\n",
    "    print('read log data file runtime (s):', end-start)\n",
    "\n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(df.page=='NextSong')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    start = time.time()\n",
    "    artists_table = df.select(\"user_id\", \"first_name\", \"last_name\", \"gender\", \"level\").distinct()\n",
    "    end = time.time()\n",
    "    print('extract columns for users table runtime (s):', end-start)\n",
    "    \n",
    "    # write users table to parquet files\n",
    "    start = time.time()\n",
    "    artists_table_parquet = df.select(\"user_id\", \"first_name\", \"lsat_name\", \"gender\", \"level\").distinct()\n",
    "    artists_table_parquet.write.mode('overwrite').partitionBy(\"year\", \"artist_id\").parquet(output_data + \"users/\")\n",
    "    end = time.time()\n",
    "    print('write users table to parquet files runtime (s):', end-start)\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = F.udf(lambda x: datetime.fromtimestamp(x/1000.0) ), T.TimestampType() )\n",
    "    df = df.withColumn(\"timestamp\", get_timestamp(df.ts)) # creates new column named timestamp, populates it with converted timestamp\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = F.udf(lambda x: datetime.fromtimestamp(x/1000.0) )\n",
    "    df = df.withColumn(\"datetime\", get_timestamp(df.ts))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = \n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_df = \n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = \n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    Create Spark session, provide paths to input/output data, load songs/log data\n",
    "    and create parquet tables (columnar format) with star schema DB.\n",
    "    '''\n",
    "    spark = create_spark_session()\n",
    "    input_data = \"s3a://udacity-dend/\"\n",
    "    output_data = \"s3a://dend-sparkify-amiri/output_data\"\n",
    "    \n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANDBOX AREA BELOW ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# This code is from the internet, and it works\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local')\n",
    "create_spark_session()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works! Stops the spark context\n",
    "SparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install boto3\n",
    "# Also had to install Java SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to udacity bucket with the console:  https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/?region=us-west-2\n",
    "#s3path = 'https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/?region=us-west-2#'\n",
    "#s3pathsong = 'https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/song_data/A/?region=us-west-2'\n"
   ]
  }
 ]
}