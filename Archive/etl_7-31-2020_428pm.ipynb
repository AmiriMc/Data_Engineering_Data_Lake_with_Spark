{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38332bit92b6213f5c2744d0ab77ccfa3a973704",
   "display_name": "Python 3.8.3 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "import  pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['dl.cfg']"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x1029e80>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://DESKTOP-GJKKHSO:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "# of file_locations: 26\n"
    }
   ],
   "source": [
    "import boto3\n",
    "bucket = 'udacity-dend'\n",
    "#Make sure you provide / in the end\n",
    "prefix = 'song_data/A/'  \n",
    "file_locations = []\n",
    "\n",
    "client = boto3.client('s3')\n",
    "result = client.list_objects(Bucket=bucket, Prefix=prefix, Delimiter='/')\n",
    "for o in result.get('CommonPrefixes'):\n",
    "    file_locations.append(o.get('Prefix'))\n",
    "\n",
    "print(\"# of file_locations:\", len( file_locations))\n",
    "file_locations = [\"s3a://udacity-dend/\" + s for s in file_locations]\n",
    "file_locations = [ s + \"*\" for s in file_locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "# of file_locations: 26\n"
    }
   ],
   "source": [
    "import boto3\n",
    "bucket = 'udacity-dend'\n",
    "#Make sure you provide / in the end\n",
    "prefix = 'song_data/A/'  \n",
    "file_locations = []\n",
    "\n",
    "client = boto3.client('s3')\n",
    "result = client.list_objects(Bucket=bucket, Prefix=prefix, Delimiter='/')\n",
    "for o in result.get('CommonPrefixes'):\n",
    "    file_locations.append(o.get('Prefix'))\n",
    "\n",
    "print(\"# of file_locations:\", len( file_locations))\n",
    "file_locations = [\"s3a://udacity-dend/\" + s for s in file_locations]\n",
    "file_locations = [ s + \"*\" for s in file_locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3path = 'https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/?region=us-west-2#'\n",
    "s3pathsong = 'https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/song_data/A/?region=us-west-2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "song_data/A/Z/D/TRAZDFN12903CE9596.json\nsong_data/A/Z/D/TRAZDFY128F4217B71.json\nsong_data/A/Z/D/TRAZDFZ128F426FC56.json\nsong_data/A/Z/D/TRAZDGW128F424A4A3.json\nsong_data/A/Z/D/TRAZDHF128F147652B.json\nsong_data/A/Z/D/TRAZDHN12903CB7532.json\nsong_data/A/Z/D/TRAZDID128F92DFE93.json\nsong_data/A/Z/D/TRAZDJA128F42282CC.json\nsong_data/A/Z/D/TRAZDJL128F147659D.json\nsong_data/A/Z/D/TRAZDMY128E078B2D6.json\nsong_data/A/Z/D/TRAZDNB128F146B048.json\nsong_data/A/Z/D/TRAZDPF12903CF129D.json\nsong_data/A/Z/D/TRAZDPO128E078ECE6.json\nsong_data/A/Z/D/TRAZDRF12903CE41F0.json\nsong_data/A/Z/D/TRAZDTL128F92D4A05.json\nsong_data/A/Z/D/TRAZDVH12903CDD5AC.json\nsong_data/A/Z/D/TRAZDYQ12903CF43F6.json\nsong_data/A/Z/D/TRAZDZB128F428CF4A.json\nsong_data/A/Z/D/TRAZDZH128E0787878.json\nsong_data/A/Z/E/TRAZEBG128F1456D2E.json\nsong_data/A/Z/E/TRAZECG128F426471C.json\nsong_data/A/Z/E/TRAZEDI128E0784866.json\nsong_data/A/Z/E/TRAZEDX128F1488A82.json\nsong_data/A/Z/E/TRAZEET128F92D98B2.json\nsong_data/A/Z/E/TRAZEGV128F1499181.json\nsong_data/A/Z/E/TRAZEHE128F9330D81.json\nsong_data/A/Z/E/TRAZEJN12903CC5E4A.json\nsong_data/A/Z/E/TRAZEKR128F4255A64.json\nsong_data/A/Z/E/TRAZEMA128F42669A2.json\nsong_data/A/Z/E/TRAZEMH128E0794A6E.json\nsong_data/A/Z/E/TRAZEMT128F424C939.json\nsong_data/A/Z/E/TRAZEOE128F93161C6.json\nsong_data/A/Z/E/TRAZEOK128F429E934.json\nsong_data/A/Z/E/TRAZEQS128F429AE5F.json\nsong_data/A/Z/E/TRAZEQU128F42926FD.json\nsong_data/A/Z/E/TRAZERQ128F92FD2E1.json\nsong_data/A/Z/E/TRAZESD128F423CB53.json\nsong_data/A/Z/E/TRAZEVO128EF341504.json\nsong_data/A/Z/E/TRAZEXE128E0790C13.json\nsong_data/A/Z/E/TRAZEXX128F92C9567.json\nsong_data/A/Z/E/TRAZEZW128F930F4FF.json\nsong_data/A/Z/E/TRAZEZX128E0787EA5.json\nsong_data/A/Z/F/TRAZFAR12903CC5E6C.json\nsong_data/A/Z/F/TRAZFAZ128F14921C3.json\nsong_data/A/Z/F/TRAZFCE128F92F5713.json\nsong_data/A/Z/F/TRAZFFW12903CEA95B.json\nsong_data/A/Z/F/TRAZFHB128F1466850.json\nsong_data/A/Z/F/TRAZFIJ128F42778AF.json\nsong_data/A/Z/F/TRAZFKN128F4250CD4.json\nsong_data/A/Z/F/TRAZFLO128F931F4DB.json\nsong_data/A/Z/F/TRAZFMD128F9305FD3.json\nsong_data/A/Z/F/TRAZFMG128F4255C6A.json\nsong_data/A/Z/F/TRAZFOU12903CEB62A.json\nsong_data/A/Z/F/TRAZFOW128F934B502.json\nsong_data/A/Z/F/TRAZFRJ128F426369D.json\nsong_data/A/Z/F/TRAZFVZ128F148EB93.json\nsong_data/A/Z/F/TRAZFWM128EF352687.json\nsong_data/A/Z/F/TRAZFYU128F93569C6.json\nsong_data/A/Z/F/TRAZFYY128F147FB43.json\nsong_data/A/Z/G/TRAZGCX128F4269165.json\nsong_data/A/Z/G/TRAZGDD128F92EDE72.json\nsong_data/A/Z/G/TRAZGDX128F92FFC68.json\nsong_data/A/Z/G/TRAZGEJ128F932ECE9.json\nsong_data/A/Z/G/TRAZGFC128F426DD92.json\nsong_data/A/Z/G/TRAZGHD128F14541C4.json\nsong_data/A/Z/G/TRAZGJQ128F92EAA7D.json\nsong_data/A/Z/G/TRAZGJX128F4263B7A.json\nsong_data/A/Z/G/TRAZGMI128F424A53B.json\nsong_data/A/Z/G/TRAZGNE128F4249483.json\nsong_data/A/Z/G/TRAZGNM12903CAA88E.json\nsong_data/A/Z/G/TRAZGOK128F92F702B.json\nsong_data/A/Z/G/TRAZGOZ128F1457072.json\nsong_data/A/Z/G/TRAZGOZ12903CD6A5F.json\nsong_data/A/Z/G/TRAZGPW128F426023D.json\nsong_data/A/Z/G/TRAZGQQ128F4278C1A.json\nsong_data/A/Z/G/TRAZGQT128F933CFA8.json\nsong_data/A/Z/G/TRAZGQT12903CE969B.json\nsong_data/A/Z/G/TRAZGRG128F148D44C.json\nsong_data/A/Z/G/TRAZGTZ128E078CF21.json\nsong_data/A/Z/G/TRAZGUK128F42845BF.json\nsong_data/A/Z/G/TRAZGWD12903CE848D.json\nsong_data/A/Z/G/TRAZGXN128F92F8C04.json\nsong_data/A/Z/G/TRAZGXO128F42850B7.json\nsong_data/A/Z/G/TRAZGXV128F146E9BA.json\nsong_data/A/Z/G/TRAZGXX12903CD8ECC.json\nsong_data/A/Z/G/TRAZGXY128F92E3F21.json\nsong_data/A/Z/G/TRAZGYL128F9339EA9.json\nsong_data/A/Z/G/TRAZGZV128F424937D.json\nsong_data/A/Z/H/TRAZHAJ128F4289AE1.json\nsong_data/A/Z/H/TRAZHBT128F42A10B4.json\nsong_data/A/Z/H/TRAZHFE128F14900F3.json\nsong_data/A/Z/H/TRAZHFL128F14795AA.json\nsong_data/A/Z/H/TRAZHHW12903CA842D.json\nsong_data/A/Z/H/TRAZHIW128F92DF2F5.json\nsong_data/A/Z/H/TRAZHKG128F4262F6F.json\nsong_data/A/Z/H/TRAZHKK128F9339EE0.json\nsong_data/A/Z/H/TRAZHMK12903CFA406.json\nsong_data/A/Z/H/TRAZHNL12903CC1B6C.json\nsong_data/A/Z/H/TRAZHRC128F9338382.json\nsong_data/A/Z/H/TRAZHRE128F92F344E.json\nsong_data/A/Z/H/TRAZHTL128F92DEB9C.json\nsong_data/A/Z/H/TRAZHTU12903CA1595.json\nsong_data/A/Z/H/TRAZHUJ128F931750A.json\nsong_data/A/Z/H/TRAZHXG128F426B2C9.json\nsong_data/A/Z/H/TRAZHXM128F146F711.json\nsong_data/A/Z/H/TRAZHYO128F42AF21A.json\nsong_data/A/Z/H/TRAZHZC128F92FDD6B.json\nsong_data/A/Z/I/TRAZIAK12903CBF3BF.json\nsong_data/A/Z/I/TRAZIBN128F1461D0B.json\nsong_data/A/Z/I/TRAZIDL128F42ADC1A.json\nsong_data/A/Z/I/TRAZIER128F4254BBA.json\nsong_data/A/Z/I/TRAZIHK12903CCC449.json\nsong_data/A/Z/I/TRAZIJH12903D006D7.json\nsong_data/A/Z/I/TRAZIJP128F1453DA8.json\nsong_data/A/Z/I/TRAZIKW128E078FB64.json\nsong_data/A/Z/I/TRAZILE128F42689F4.json\nsong_data/A/Z/I/TRAZINI128F42470FE.json\nsong_data/A/Z/I/TRAZIPB128F427CAFB.json\nsong_data/A/Z/I/TRAZIQB128F934B755.json\nsong_data/A/Z/I/TRAZIQK12903CCFB3A.json\nsong_data/A/Z/I/TRAZIQO128F93026CF.json\nsong_data/A/Z/I/TRAZISB128F427907A.json\nsong_data/A/Z/I/TRAZIUP12903CC1CDD.json\nsong_data/A/Z/I/TRAZIWR128F4234282.json\nsong_data/A/Z/I/TRAZIWW128F146AF43.json\nsong_data/A/Z/J/TRAZJBN12903CE4E74.json\nsong_data/A/Z/J/TRAZJCU12903CD062B.json\nsong_data/A/Z/J/TRAZJEF128F4251F65.json\nsong_data/A/Z/J/TRAZJFR12903CEBB8F.json\nsong_data/A/Z/J/TRAZJFT128F4257871.json\nsong_data/A/Z/J/TRAZJGD128F4289D2A.json\nsong_data/A/Z/J/TRAZJGR128F4237073.json\nsong_data/A/Z/J/TRAZJIF128F14853C9.json\nsong_data/A/Z/J/TRAZJIL12903C9D1D0.json\nsong_data/A/Z/J/TRAZJJX12903CE882B.json\nsong_data/A/Z/J/TRAZJLG128F4242DDC.json\nsong_data/A/Z/J/TRAZJMK12903CBEE95.json\nsong_data/A/Z/J/TRAZJMN128F148A5B8.json\nsong_data/A/Z/J/TRAZJNI128F14841E0.json\nsong_data/A/Z/J/TRAZJNS128F92C3ABA.json\nsong_data/A/Z/J/TRAZJNW12903CEA26C.json\nsong_data/A/Z/J/TRAZJRK12903D07223.json\nsong_data/A/Z/J/TRAZJTT128F9308F94.json\nsong_data/A/Z/J/TRAZJXA128F421393B.json\nsong_data/A/Z/J/TRAZJXK12903CEE4AA.json\nsong_data/A/Z/J/TRAZJXO128EF355468.json\nsong_data/A/Z/J/TRAZJZP128F427FFAC.json\nsong_data/A/Z/K/TRAZKAG12903CD1918.json\nsong_data/A/Z/K/TRAZKCQ128F145D7BF.json\nsong_data/A/Z/K/TRAZKCX128F4288F26.json\nsong_data/A/Z/K/TRAZKCZ128F930C6E0.json\nsong_data/A/Z/K/TRAZKEM128E078801D.json\nsong_data/A/Z/K/TRAZKET128F426C3F3.json\nsong_data/A/Z/K/TRAZKHE128F42773B1.json\nsong_data/A/Z/K/TRAZKIO128F9317739.json\nsong_data/A/Z/K/TRAZKIS12903CEED76.json\nsong_data/A/Z/K/TRAZKJI128F92EAA44.json\nsong_data/A/Z/K/TRAZKJL128F42733D2.json\nsong_data/A/Z/K/TRAZKJU128F92C972A.json\nsong_data/A/Z/K/TRAZKLY12903CAE4D7.json\nsong_data/A/Z/K/TRAZKMQ128F92EAE0E.json\nsong_data/A/Z/K/TRAZKPM12903CE9664.json\nsong_data/A/Z/K/TRAZKSH128C7196374.json\nsong_data/A/Z/K/TRAZKSI128F147B0CF.json\nsong_data/A/Z/K/TRAZKTZ128F425C690.json\nsong_data/A/Z/K/TRAZKTZ128F9329824.json\nsong_data/A/Z/K/TRAZKVT12903CF2948.json\nsong_data/A/Z/K/TRAZKWA128F429539B.json\nsong_data/A/Z/K/TRAZKXE128F149437C.json\nsong_data/A/Z/K/TRAZKZG128F9329D11.json\nsong_data/A/Z/K/TRAZKZW128E0785636.json\nsong_data/A/Z/L/TRAZLCH12903CE3D6D.json\nsong_data/A/Z/L/TRAZLEG128EF35F4C8.json\nsong_data/A/Z/L/TRAZLEV12903CEAA40.json\nsong_data/A/Z/L/TRAZLFE128F4247436.json\nsong_data/A/Z/L/TRAZLFO12903CB6C0B.json\nsong_data/A/Z/L/TRAZLHO128F42BC98A.json\nsong_data/A/Z/L/TRAZLII128F42BC731.json\nsong_data/A/Z/L/TRAZLIN128F14ABC02.json\nsong_data/A/Z/L/TRAZLIO128F92F1DB0.json\nsong_data/A/Z/L/TRAZLIS128F1486F43.json\nsong_data/A/Z/L/TRAZLIT12903CA1372.json\nsong_data/A/Z/L/TRAZLIW128F92E3246.json\nsong_data/A/Z/L/TRAZLKJ128E0791C37.json\nsong_data/A/Z/L/TRAZLKJ12903CDBA79.json\nsong_data/A/Z/L/TRAZLMM128F932274C.json\nsong_data/A/Z/L/TRAZLMS128F1481E53.json\nsong_data/A/Z/L/TRAZLNE128E0793C7B.json\nsong_data/A/Z/L/TRAZLOG12903CF1086.json\nsong_data/A/Z/L/TRAZLTA128F425510D.json\nsong_data/A/Z/L/TRAZLVO128F148CABE.json\nsong_data/A/Z/L/TRAZLXF128F92FECF9.json\nsong_data/A/Z/L/TRAZLXO128F428B8BE.json\nsong_data/A/Z/L/TRAZLZM12903CFB584.json\nsong_data/A/Z/M/TRAZMCH128F4266B39.json\nsong_data/A/Z/M/TRAZMCI128F92F81B3.json\nsong_data/A/Z/M/TRAZMCS128F9345574.json\nsong_data/A/Z/M/TRAZMDC128F92F7AA8.json\nsong_data/A/Z/M/TRAZMDJ128E0798AA4.json\nsong_data/A/Z/M/TRAZMFD128F92EE193.json\nsong_data/A/Z/M/TRAZMFE128F92FAD71.json\nsong_data/A/Z/M/TRAZMFV128F933FECD.json\nsong_data/A/Z/M/TRAZMGL128F421546B.json\nsong_data/A/Z/M/TRAZMHT128F4296C31.json\nsong_data/A/Z/M/TRAZMKM128F9331D63.json\nsong_data/A/Z/M/TRAZMLR128F146B237.json\nsong_data/A/Z/M/TRAZMLU128F423EB96.json\nsong_data/A/Z/M/TRAZMMK128F9309025.json\nsong_data/A/Z/M/TRAZMND128F4269762.json\nsong_data/A/Z/M/TRAZMOA12903CA0B85.json\nsong_data/A/Z/M/TRAZMOD128F92E4685.json\nsong_data/A/Z/M/TRAZMPA128F145F17E.json\nsong_data/A/Z/M/TRAZMSP12903CC9C8E.json\nsong_data/A/Z/M/TRAZMVK128F4235AF8.json\nsong_data/A/Z/M/TRAZMVZ128F42658B0.json\nsong_data/A/Z/M/TRAZMZY128E078F06A.json\nsong_data/A/Z/N/TRAZNAD128F9333A54.json\nsong_data/A/Z/N/TRAZNCC12903CC45A9.json\nsong_data/A/Z/N/TRAZNDI128F9321777.json\nsong_data/A/Z/N/TRAZNFE128F92E49BF.json\nsong_data/A/Z/N/TRAZNGB128F425A305.json\nsong_data/A/Z/N/TRAZNGN128EF34B644.json\nsong_data/A/Z/N/TRAZNIE128F93506C3.json\nsong_data/A/Z/N/TRAZNIU128F421F5B8.json\nsong_data/A/Z/N/TRAZNJG128F42326A4.json\nsong_data/A/Z/N/TRAZNKG12903CDCF8A.json\nsong_data/A/Z/N/TRAZNLA128F930A44E.json\nsong_data/A/Z/N/TRAZNOM128F9325295.json\nsong_data/A/Z/N/TRAZNPQ128F9325E16.json\nsong_data/A/Z/N/TRAZNQU128F1489D2A.json\nsong_data/A/Z/N/TRAZNWK128F92DB7F2.json\nsong_data/A/Z/N/TRAZNXI128F92E7034.json\nsong_data/A/Z/N/TRAZNZB128F4214C97.json\nsong_data/A/Z/O/TRAZOAE128F9347A40.json\nsong_data/A/Z/O/TRAZOAW128F4250AA9.json\nsong_data/A/Z/O/TRAZOBA128F9303210.json\nsong_data/A/Z/O/TRAZOBD128F93231ED.json\nsong_data/A/Z/O/TRAZOBV128F4226F73.json\nsong_data/A/Z/O/TRAZOCB12903CDA934.json\nsong_data/A/Z/O/TRAZOCD128F4249BB5.json\nsong_data/A/Z/O/TRAZOCM128F4270F94.json\nsong_data/A/Z/O/TRAZODO128F930E43C.json\nsong_data/A/Z/O/TRAZOEN128F933339A.json\nsong_data/A/Z/O/TRAZOEU128F92E28DD.json\nsong_data/A/Z/O/TRAZOFB128F4278424.json\nsong_data/A/Z/O/TRAZOFU128EF361C38.json\nsong_data/A/Z/O/TRAZOGQ128F14768A3.json\nsong_data/A/Z/O/TRAZOIB128F934611F.json\nsong_data/A/Z/O/TRAZOID128F9327AE1.json\nsong_data/A/Z/O/TRAZOIX128F428B137.json\nsong_data/A/Z/O/TRAZOJV128F42791C2.json\nsong_data/A/Z/O/TRAZOJW128F42A3644.json\nsong_data/A/Z/O/TRAZOKL128F424304A.json\nsong_data/A/Z/O/TRAZONP128F147D8E8.json\nsong_data/A/Z/O/TRAZOOS128F423247E.json\nsong_data/A/Z/O/TRAZOOS128F424EF7A.json\nsong_data/A/Z/O/TRAZOQJ128F4285961.json\nsong_data/A/Z/O/TRAZORC128F426C323.json\nsong_data/A/Z/O/TRAZOSC128F14ACECD.json\nsong_data/A/Z/O/TRAZOSO128F149739D.json\nsong_data/A/Z/O/TRAZOWT128F1489C92.json\nsong_data/A/Z/O/TRAZOWT12903CE82FB.json\nsong_data/A/Z/O/TRAZOXK128F92D24F6.json\nsong_data/A/Z/P/TRAZPCR128F932739B.json\nsong_data/A/Z/P/TRAZPEJ128EF33E9D1.json\nsong_data/A/Z/P/TRAZPEV128F92F9A2E.json\nsong_data/A/Z/P/TRAZPHA128F1483883.json\nsong_data/A/Z/P/TRAZPHS128F92DE6E0.json\nsong_data/A/Z/P/TRAZPIY128F932B091.json\nsong_data/A/Z/P/TRAZPJH128F4260EFC.json\nsong_data/A/Z/P/TRAZPJO12903CB4EA6.json\nsong_data/A/Z/P/TRAZPOG128F92E45C2.json\nsong_data/A/Z/P/TRAZPPA128F422DDDD.json\nsong_data/A/Z/P/TRAZPPT128F428F511.json\nsong_data/A/Z/P/TRAZPRY128F1479378.json\nsong_data/A/Z/P/TRAZPVB128F932D167.json\nsong_data/A/Z/P/TRAZPVO128F9343172.json\nsong_data/A/Z/P/TRAZPYZ12903CE59DF.json\nsong_data/A/Z/Q/TRAZQAM12903CD173E.json\nsong_data/A/Z/Q/TRAZQCY128F933C20E.json\nsong_data/A/Z/Q/TRAZQDU128F934B17B.json\nsong_data/A/Z/Q/TRAZQET128F42A0449.json\nsong_data/A/Z/Q/TRAZQHM12903C97CD4.json\nsong_data/A/Z/Q/TRAZQHP12903D1015E.json\nsong_data/A/Z/Q/TRAZQIG128F146372E.json\nsong_data/A/Z/Q/TRAZQIL12903CD2331.json\nsong_data/A/Z/Q/TRAZQJI128F147B4BB.json\nsong_data/A/Z/Q/TRAZQKD128F146A4D7.json\nsong_data/A/Z/Q/TRAZQLH12903D1403E.json\nsong_data/A/Z/Q/TRAZQPH128F92F5FEF.json\nsong_data/A/Z/Q/TRAZQQH128F92F6B3F.json\nsong_data/A/Z/Q/TRAZQQV128F428BAB5.json\nsong_data/A/Z/Q/TRAZQRV12903D08226.json\nsong_data/A/Z/Q/TRAZQSU128F428C6F1.json\nsong_data/A/Z/Q/TRAZQSX128F933AD5A.json\nsong_data/A/Z/Q/TRAZQTC128F931BCBD.json\nsong_data/A/Z/Q/TRAZQTY12903CEDACC.json\nsong_data/A/Z/Q/TRAZQXG128F148B621.json\nsong_data/A/Z/R/TRAZRAR128F1477778.json\nsong_data/A/Z/R/TRAZRBF128F9317BF5.json\nsong_data/A/Z/R/TRAZRBR128E0782989.json\nsong_data/A/Z/R/TRAZRER128F92F53BC.json\nsong_data/A/Z/R/TRAZRGG12903CF062C.json\nsong_data/A/Z/R/TRAZRGQ12903CB42A7.json\nsong_data/A/Z/R/TRAZRGR12903CE2DFC.json\nsong_data/A/Z/R/TRAZRKI128F426A244.json\nsong_data/A/Z/R/TRAZRKM128F4289DBB.json\nsong_data/A/Z/R/TRAZRKS128F147099D.json\nsong_data/A/Z/R/TRAZRQQ128F9318569.json\nsong_data/A/Z/R/TRAZRQW128F146AAA2.json\nsong_data/A/Z/R/TRAZRRR128F92EC8F8.json\nsong_data/A/Z/R/TRAZRRY128F931BABD.json\nsong_data/A/Z/R/TRAZRSG128F92D1F2C.json\nsong_data/A/Z/R/TRAZRSZ12903CA5BDF.json\nsong_data/A/Z/R/TRAZRUW128F4269ECF.json\nsong_data/A/Z/R/TRAZRVN12903CC1925.json\nsong_data/A/Z/R/TRAZRVO12903CF68CE.json\nsong_data/A/Z/R/TRAZRZY128F429F70E.json\nsong_data/A/Z/S/TRAZSBF128F933BBCA.json\nsong_data/A/Z/S/TRAZSCZ128F42554F4.json\nsong_data/A/Z/S/TRAZSDN128F42B65CD.json\nsong_data/A/Z/S/TRAZSEV128F932A319.json\nsong_data/A/Z/S/TRAZSFM128F933B19A.json\nsong_data/A/Z/S/TRAZSFV128F14806FE.json\nsong_data/A/Z/S/TRAZSGB128F42359DA.json\nsong_data/A/Z/S/TRAZSIH128F429F230.json\nsong_data/A/Z/S/TRAZSJN128F429713A.json\nsong_data/A/Z/S/TRAZSJY128F92CCEA0.json\nsong_data/A/Z/S/TRAZSLO128F42887A0.json\nsong_data/A/Z/S/TRAZSLO12903CE8ADF.json\nsong_data/A/Z/S/TRAZSMH128F422688D.json\nsong_data/A/Z/S/TRAZSMO128F425C801.json\nsong_data/A/Z/S/TRAZSNG128F9326B25.json\nsong_data/A/Z/S/TRAZSPX128F4275112.json\nsong_data/A/Z/S/TRAZSRC128F425A06C.json\nsong_data/A/Z/S/TRAZSRJ128F4216E54.json\nsong_data/A/Z/S/TRAZSRV128F1452866.json\nsong_data/A/Z/S/TRAZSRX128F92F2B42.json\nsong_data/A/Z/S/TRAZSSF128F9304D78.json\nsong_data/A/Z/S/TRAZSUU12903CC4673.json\nsong_data/A/Z/S/TRAZSVI128F4275EEA.json\nsong_data/A/Z/S/TRAZSVI12903CEDB48.json\nsong_data/A/Z/S/TRAZSVJ128E0785129.json\nsong_data/A/Z/S/TRAZSVS128F42A9051.json\nsong_data/A/Z/S/TRAZSWR128F421448B.json\nsong_data/A/Z/S/TRAZSXD128F425EA5B.json\nsong_data/A/Z/S/TRAZSXO12903CC26AF.json\nsong_data/A/Z/T/TRAZTBZ128F4274A59.json\nsong_data/A/Z/T/TRAZTBZ12903CFB6DA.json\nsong_data/A/Z/T/TRAZTCQ128F42482A3.json\nsong_data/A/Z/T/TRAZTDM12903CD2029.json\nsong_data/A/Z/T/TRAZTEJ128EF351AD3.json\nsong_data/A/Z/T/TRAZTEK128F145C90E.json\nsong_data/A/Z/T/TRAZTHP128F145849C.json\nsong_data/A/Z/T/TRAZTIW12903CBB895.json\nsong_data/A/Z/T/TRAZTNI128F931ABF6.json\nsong_data/A/Z/T/TRAZTQC128F1463E65.json\nsong_data/A/Z/T/TRAZTQO12903CC1BB3.json\nsong_data/A/Z/T/TRAZTSJ128F4260057.json\nsong_data/A/Z/T/TRAZTST128F93355A0.json\nsong_data/A/Z/T/TRAZTUE128F428A3A1.json\nsong_data/A/Z/T/TRAZTWI128F424CF73.json\nsong_data/A/Z/T/TRAZTYQ128F149025A.json\nsong_data/A/Z/U/TRAZUAF128F9300CFC.json\nsong_data/A/Z/U/TRAZUBE128F93199AD.json\nsong_data/A/Z/U/TRAZUBL128F92F0BF1.json\nsong_data/A/Z/U/TRAZUCD12903CF5A98.json\nsong_data/A/Z/U/TRAZUCE128F425582C.json\nsong_data/A/Z/U/TRAZUFL128F92F992B.json\nsong_data/A/Z/U/TRAZUGY12903CD8E1E.json\nsong_data/A/Z/U/TRAZUKS12903CE54B9.json\nsong_data/A/Z/U/TRAZULH12903CB30B2.json\nsong_data/A/Z/U/TRAZUMX128F145ABA1.json\nsong_data/A/Z/U/TRAZUND12903CA9CBF.json\nsong_data/A/Z/U/TRAZUOG128F4259EAA.json\nsong_data/A/Z/U/TRAZUOJ128F92EB81F.json\nsong_data/A/Z/U/TRAZUQL128F92CEE22.json\nsong_data/A/Z/U/TRAZUQM128F4242779.json\nsong_data/A/Z/U/TRAZURS128F14A1CD2.json\nsong_data/A/Z/U/TRAZUTX128F147FCE8.json\nsong_data/A/Z/U/TRAZUUN128F92D318A.json\nsong_data/A/Z/U/TRAZUVR128E0789ACB.json\nsong_data/A/Z/U/TRAZUVY128F93018AB.json\nsong_data/A/Z/U/TRAZUXK128F1474CC1.json\nsong_data/A/Z/U/TRAZUXK128F42359FA.json\nsong_data/A/Z/U/TRAZUYS128F148EB8A.json\nsong_data/A/Z/V/TRAZVAT128F92F9223.json\nsong_data/A/Z/V/TRAZVBF128F92E780E.json\nsong_data/A/Z/V/TRAZVCP128F425F3EA.json\nsong_data/A/Z/V/TRAZVDA128F92D7E65.json\nsong_data/A/Z/V/TRAZVEJ128F92EDA77.json\nsong_data/A/Z/V/TRAZVFJ128F9310CE2.json\nsong_data/A/Z/V/TRAZVHK128F930032C.json\nsong_data/A/Z/V/TRAZVIA128F427F1CF.json\nsong_data/A/Z/V/TRAZVKH128F146E6F5.json\nsong_data/A/Z/V/TRAZVKZ128E0781793.json\nsong_data/A/Z/V/TRAZVLD128F92F7335.json\nsong_data/A/Z/V/TRAZVMD128F14613E6.json\nsong_data/A/Z/V/TRAZVNV128F92D6FA8.json\nsong_data/A/Z/V/TRAZVPM12903CA7F32.json\nsong_data/A/Z/V/TRAZVPN128F149012A.json\nsong_data/A/Z/V/TRAZVQI128F4262277.json\nsong_data/A/Z/V/TRAZVSM128F92D541F.json\nsong_data/A/Z/V/TRAZVWR12903CD9EFE.json\nsong_data/A/Z/V/TRAZVXH128F9327600.json\nsong_data/A/Z/V/TRAZVYP128F1459AD9.json\nsong_data/A/Z/V/TRAZVZJ128F92EF28A.json\nsong_data/A/Z/W/TRAZWCB128F4256456.json\nsong_data/A/Z/W/TRAZWFP128F425C6D0.json\nsong_data/A/Z/W/TRAZWGV128F425DA7C.json\nsong_data/A/Z/W/TRAZWHQ128F4255B14.json\nsong_data/A/Z/W/TRAZWIB128F92FA059.json\nsong_data/A/Z/W/TRAZWJE128F9344D6E.json\nsong_data/A/Z/W/TRAZWJK128E0792AEA.json\nsong_data/A/Z/W/TRAZWJN128F9350E66.json\nsong_data/A/Z/W/TRAZWJQ12903C9AE3D.json\nsong_data/A/Z/W/TRAZWKJ12903CE945C.json\nsong_data/A/Z/W/TRAZWNP128F932408A.json\nsong_data/A/Z/W/TRAZWOA128F147A834.json\nsong_data/A/Z/W/TRAZWOA128F932D49A.json\nsong_data/A/Z/W/TRAZWOJ128F4289269.json\nsong_data/A/Z/W/TRAZWQY128F92F7F1C.json\nsong_data/A/Z/W/TRAZWRJ128F426C38E.json\nsong_data/A/Z/W/TRAZWRN128F9337576.json\nsong_data/A/Z/W/TRAZWRQ128E0781E81.json\nsong_data/A/Z/W/TRAZWSL128F4288BFF.json\nsong_data/A/Z/W/TRAZWSN12903CD494F.json\nsong_data/A/Z/W/TRAZWSO128F9343D0B.json\nsong_data/A/Z/W/TRAZWTE128F9354A87.json\nsong_data/A/Z/W/TRAZWTP128F424C226.json\nsong_data/A/Z/W/TRAZWTU128F427B768.json\nsong_data/A/Z/W/TRAZWUI128F425ECCA.json\nsong_data/A/Z/W/TRAZWXR128F4290CEE.json\nsong_data/A/Z/W/TRAZWZE128F92EC86D.json\nsong_data/A/Z/W/TRAZWZG128F932E4A4.json\nsong_data/A/Z/X/TRAZXAZ12903CA125B.json\nsong_data/A/Z/X/TRAZXBD12903CD6EC8.json\nsong_data/A/Z/X/TRAZXBG12903CC5BF2.json\nsong_data/A/Z/X/TRAZXBO128F429DA70.json\nsong_data/A/Z/X/TRAZXDB128F4249DB5.json\nsong_data/A/Z/X/TRAZXDC128F429874E.json\nsong_data/A/Z/X/TRAZXDC12903CF0476.json\nsong_data/A/Z/X/TRAZXEG128F42685E7.json\nsong_data/A/Z/X/TRAZXGR128F930EE23.json\nsong_data/A/Z/X/TRAZXIB12903CA6BD1.json\nsong_data/A/Z/X/TRAZXIO128F4253C20.json\nsong_data/A/Z/X/TRAZXLP12903CC2515.json\nsong_data/A/Z/X/TRAZXNN128F42BC73E.json\nsong_data/A/Z/X/TRAZXUT128F422EF3C.json\nsong_data/A/Z/X/TRAZXVB128F4271EF9.json\nsong_data/A/Z/X/TRAZXVF128F1459C44.json\nsong_data/A/Z/X/TRAZXYC12903D07493.json\nsong_data/A/Z/X/TRAZXZJ128EF35C86D.json\nsong_data/A/Z/Y/TRAZYAU12903CBAC78.json\nsong_data/A/Z/Y/TRAZYAX128F9329061.json\nsong_data/A/Z/Y/TRAZYBC128F4214A10.json\nsong_data/A/Z/Y/TRAZYCJ128F4223185.json\nsong_data/A/Z/Y/TRAZYCV128F9332B23.json\nsong_data/A/Z/Y/TRAZYDK128F4266508.json\nsong_data/A/Z/Y/TRAZYFU128F4299109.json\nsong_data/A/Z/Y/TRAZYHY128F92D2C2E.json\nsong_data/A/Z/Y/TRAZYIG128F42893B9.json\nsong_data/A/Z/Y/TRAZYIV128F429F1C5.json\nsong_data/A/Z/Y/TRAZYJA12903CF5F45.json\nsong_data/A/Z/Y/TRAZYJG128F421CC99.json\nsong_data/A/Z/Y/TRAZYJZ12903CF5359.json\nsong_data/A/Z/Y/TRAZYOU128F92C3FA1.json\nsong_data/A/Z/Y/TRAZYQF12903CE3DA5.json\nsong_data/A/Z/Y/TRAZYRC128E0792FC7.json\nsong_data/A/Z/Y/TRAZYRO12903CA5029.json\nsong_data/A/Z/Y/TRAZYRZ12903CC0103.json\nsong_data/A/Z/Y/TRAZYSO128F9300356.json\nsong_data/A/Z/Y/TRAZYVU128F932915C.json\nsong_data/A/Z/Y/TRAZYWG128F425EB49.json\nsong_data/A/Z/Y/TRAZYXD128F4239F02.json\nsong_data/A/Z/Y/TRAZYXX128F4276DDF.json\nsong_data/A/Z/Y/TRAZYYB12903CE5E46.json\nsong_data/A/Z/Y/TRAZYZF128F423FAD6.json\nsong_data/A/Z/Y/TRAZYZQ128F146653D.json\nsong_data/A/Z/Z/TRAZZAV12903CAC9F4.json\nsong_data/A/Z/Z/TRAZZAY128F148209C.json\nsong_data/A/Z/Z/TRAZZCJ128F42373BB.json\nsong_data/A/Z/Z/TRAZZDD128F92CCBD6.json\nsong_data/A/Z/Z/TRAZZEE128F92E1A83.json\nsong_data/A/Z/Z/TRAZZFE128F146A702.json\nsong_data/A/Z/Z/TRAZZHJ128F931F37A.json\nsong_data/A/Z/Z/TRAZZJK128F423683C.json\nsong_data/A/Z/Z/TRAZZKH12903CFA20A.json\nsong_data/A/Z/Z/TRAZZLJ128E07945F2.json\nsong_data/A/Z/Z/TRAZZMI12903CA6A0A.json\nsong_data/A/Z/Z/TRAZZNS128F930397F.json\nsong_data/A/Z/Z/TRAZZON128F930D2DC.json\nsong_data/A/Z/Z/TRAZZQC128F427A0F5.json\nsong_data/A/Z/Z/TRAZZSC128F934D3AF.json\nsong_data/A/Z/Z/TRAZZUF128F427673F.json\nsong_data/A/Z/Z/TRAZZUM128F4288C2A.json\nsong_data/A/Z/Z/TRAZZVZ128F9326FE1.json\nsong_data/A/Z/Z/TRAZZWL128F4239037.json\nsong_data/A/Z/Z/TRAZZXF128F4247094.json\nsong_data/A/Z/Z/TRAZZXH128F933D2BA.json\n"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "udacity_bucket = s3.Bucket('udacity-dend')\n",
    "songs_paths = []\n",
    "\n",
    "for object_summary in udacity_bucket.objects.filter(Prefix=prefix):\n",
    "    songs_paths.append(object_summary.key)\n",
    "    print(object_summary.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'list'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-7489abd72681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbucket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'list'"
     ]
    }
   ],
   "source": [
    "for key in bucket.list(prefix=prefix): \n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'objects'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-580f57e1a6b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mmy_bucket_object\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbucket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_bucket_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'objects'"
     ]
    }
   ],
   "source": [
    "for my_bucket_object in bucket.objects.all():\n",
    "    print(my_bucket_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'list_objects'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-22cbeb867389>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbucket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'list_objects'"
     ]
    }
   ],
   "source": [
    "bucket.list_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure that second line is faster than first\n",
    "#df = spark.read.format(\"json\").load(\"s3a://udacity-dend/song_data/*/*/*\")\n",
    "df = spark.read.format(\"json\").load(file_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pyspark.sql.dataframe.DataFrame'>\n"
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"s3://udacity-dend/song_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pyspark.sql.dataframe.DataFrame'>\nDataFrame[artist_id: string, artist_latitude: double, artist_location: string, artist_longitude: double, artist_name: string, duration: double, num_songs: bigint, song_id: string, title: string, year: bigint]\ncount 14896\nroot\n |-- artist_id: string (nullable = true)\n |-- artist_latitude: double (nullable = true)\n |-- artist_location: string (nullable = true)\n |-- artist_longitude: double (nullable = true)\n |-- artist_name: string (nullable = true)\n |-- duration: double (nullable = true)\n |-- num_songs: long (nullable = true)\n |-- song_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- year: long (nullable = true)\n\n"
    }
   ],
   "source": [
    "print(type(df))\n",
    "print(df)\n",
    "print('count', df.count())\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- artist_id: string (nullable = true)\n |-- artist_latitude: double (nullable = true)\n |-- artist_location: string (nullable = true)\n |-- artist_longitude: double (nullable = true)\n |-- artist_name: string (nullable = true)\n |-- duration: double (nullable = true)\n |-- num_songs: long (nullable = true)\n |-- song_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- year: long (nullable = true)\n\n"
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(\"s3a://udacity-dend/\", \"song_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import boto3 \n",
    "s3 = boto3.client(\"s3\")\n",
    "all_objects = s3.list_objects(Bucket = 'udacity-dend')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import boto3 \n",
    "s3 = boto3.client(\"s3\")\n",
    "response = s3.list_objects_v2(\n",
    "    Bucket='udacity-dend',\n",
    "    Prefix ='song_data/A',\n",
    "    MaxKeys=100 )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "#song_data = \"s3://udacity-dend/song_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.json(\"s3a://udacity-dend/song_data/A/B/C/TRABCEI128F424C983.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(type(df))\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.json(\"s3a://udacity-dend/song_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#song_data = \"s3a://udacity-dend/song_data/\"\n",
    "#df = spark.read.load(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_songs = df.withColumn(\"songplay_id\", col(\"song_id\"))\n",
    "#df_songs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\", col(\"artist_name\").alias(\"artist\")).distinct() # interesting...when I use .show(), i can't seem to write to a variable, almost as if its\n",
    "# print statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pyspark.sql.dataframe.DataFrame'>\n"
    }
   ],
   "source": [
    "print(type(songs_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------------------+--------------------+------------------+----+---------+\n|           song_id|               title|         artist_id|year| duration|\n+------------------+--------------------+------------------+----+---------+\n|SOLLALT12A8C1399F3|Piano Concerto No...|ARWMEJW11F4C83C123|   0|319.37261|\n|SOAGZUH12A6D4FB4C5|The Sparrows And ...|AR4YEJU1187B991468|1991|191.26812|\n|SOSJVZR12AB018698F|Four Simple Steps...|ARJ8IIY1187B992734|2003|  58.8273|\n|SORAUDC12A6D4F7273|The Debt Is Settl...|ARJF4E41187FB499FF|1978|233.11628|\n|SONXXYP12A81C238EE|Don't Waste My Ti...|ARZNYC61187B9958CF|1985|324.85832|\n+------------------+--------------------+------------------+----+---------+\nonly showing top 5 rows\n\n"
    }
   ],
   "source": [
    "#songs_table.show(5) # TOO SLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.selectExpr(\"pos.metadata['song_id'] as songplay_id\").limit(5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\ndf.createOrReplaceTempView(\"songtable\")\\n#spark.sql(\"\"\" CREATE TABLE IF NOT EXISTS songs (song_id STRING, title STRING, artist_id STRING, year INT, duration float)\"\"\")\\nspark.sql(\"\"\"\\n    \\n    SELECT song_id, title, artist_id, year, duration \\n    FROM songtable\\n\"\"\").show()\\n'"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "'''\n",
    "df.createOrReplaceTempView(\"songtable\")\n",
    "#spark.sql(\"\"\" CREATE TABLE IF NOT EXISTS songs (song_id STRING, title STRING, artist_id STRING, year INT, duration float)\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "    \n",
    "    SELECT song_id, title, artist_id, year, duration \n",
    "    FROM songtable\n",
    "\"\"\").show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o87.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:829)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 6.0 failed 1 times, most recent failure: Lost task 4.0 in stage 6.0 (TID 12079, DESKTOP-GJKKHSO, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\amiri\\Documents\\GitHub\\Data_Engineering_Data_Lake_with_Spark\\songs_table_parquet.parquet\\_temporary\\0\\_temporary\\attempt_20200731153202_0006_m_000004_12079\\year=0\\artist=Alberto%20Plaza\\part-00004-568ba9d1-76c4-4e1e-a617-39e272ea7a5e.c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:273)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\r\n\t... 9 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\r\n\t... 33 more\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\amiri\\Documents\\GitHub\\Data_Engineering_Data_Lake_with_Spark\\songs_table_parquet.parquet\\_temporary\\0\\_temporary\\attempt_20200731153202_0006_m_000004_12079\\year=0\\artist=Alberto%20Plaza\\part-00004-568ba9d1-76c4-4e1e-a617-39e272ea7a5e.c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:273)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\r\n\t... 9 more\r\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-29c4e93f5781>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msongs_table_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msongs_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"year\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"artist\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"songs_table_parquet.parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o87.parquet.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:829)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 6.0 failed 1 times, most recent failure: Lost task 4.0 in stage 6.0 (TID 12079, DESKTOP-GJKKHSO, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\amiri\\Documents\\GitHub\\Data_Engineering_Data_Lake_with_Spark\\songs_table_parquet.parquet\\_temporary\\0\\_temporary\\attempt_20200731153202_0006_m_000004_12079\\year=0\\artist=Alberto%20Plaza\\part-00004-568ba9d1-76c4-4e1e-a617-39e272ea7a5e.c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:273)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\r\n\t... 9 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\r\n\t... 33 more\r\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:291)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:205)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\r\n\t... 1 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\amiri\\Documents\\GitHub\\Data_Engineering_Data_Lake_with_Spark\\songs_table_parquet.parquet\\_temporary\\0\\_temporary\\attempt_20200731153202_0006_m_000004_12079\\year=0\\artist=Alberto%20Plaza\\part-00004-568ba9d1-76c4-4e1e-a617-39e272ea7a5e.c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:773)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:869)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:852)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.newOutputWriter(FileFormatDataWriter.scala:241)\r\n\tat org.apache.spark.sql.execution.datasources.DynamicPartitionDataWriter.write(FileFormatDataWriter.scala:262)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:273)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:281)\r\n\t... 9 more\r\n"
     ]
    }
   ],
   "source": [
    "songs_table_p = songs_table.write.partitionBy(\"year\", \"artist\").parquet(\"songs_table_parquet.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    '''\n",
    "    This function reads the data from S3, processes the song data using Spark, and writes the processed data back to S3.\n",
    "    '''\n",
    "    # get filepath to song data file\n",
    "    song_data = file_locations # might need to add all of the logic here\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.format(\"json\").load(file_locations)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\").distinct()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table = df.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\", col(\"artist_name\").alias(\"artist\")).distinct()\n",
    "    songs_table = songs_table.write.partitionBy(\"year\", \"artist\").parquet(\"song_table_parquet.parquet\")\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select(\"artist_id\", \"artist_name\", \"location\", \"latitidue\", \"longitude\").distinct()\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table = artists_table.write.parquet(\"artists_table_parquet.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "   '''\n",
    "    This function reads the data from S3, processes the log data using Spark, and writes the processed data back to S3.\n",
    "   '''\n",
    "    # get filepath to log data file\n",
    "    log_data = 's3://udacity-dend/log_data/'\n",
    "\n",
    "    # read log data file\n",
    "    df = \n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = \n",
    "\n",
    "    # extract columns for users table    \n",
    "    artists_table = \n",
    "    \n",
    "    # write users table to parquet files\n",
    "    artists_table\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf()\n",
    "    df = \n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf()\n",
    "    df = \n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = \n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_df = \n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = \n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    input_data = \"s3a://udacity-dend/\"\n",
    "    output_data = \"\"\n",
    "    \n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANDBOX AREA BELOW ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# This code is from the internet, and it works\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local')\n",
    "create_spark_session()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works! Stops the spark context\n",
    "SparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data = \"s3a://udacity-dend/song_data/\"\n",
    "# read song data file\n",
    "df = spark.read.load(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'song_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-af8c7c4154cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msong_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'song_data' is not defined"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "for file in song_data.objects.all():\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install boto3\n",
    "# Also had to install Java SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to udacity bucket with the console:  https://s3.console.aws.amazon.com/s3/buckets/udacity-dend/?region=us-west-2"
   ]
  }
 ]
}